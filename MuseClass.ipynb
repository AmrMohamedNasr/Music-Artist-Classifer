{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuseClass.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHHNAVlh7Zx2",
        "colab_type": "code",
        "outputId": "bc3ac464-641c-4a2b-806d-f1af80ee44f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1LJiWuxCiYjhwQfEy6SmhlFfU84oGgEfu' -O src.zip\n",
        "!unzip src.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-12 20:14:16--  https://docs.google.com/uc?export=download&id=1LJiWuxCiYjhwQfEy6SmhlFfU84oGgEfu\n",
            "Resolving docs.google.com (docs.google.com)... 216.58.197.206, 2404:6800:4004:80e::200e\n",
            "Connecting to docs.google.com (docs.google.com)|216.58.197.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/1b4t834i4138osp2p97dl6d6ldaj19mb/1557691200000/10632806613498870968/*/1LJiWuxCiYjhwQfEy6SmhlFfU84oGgEfu?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-05-12 20:14:16--  https://doc-00-2o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/1b4t834i4138osp2p97dl6d6ldaj19mb/1557691200000/10632806613498870968/*/1LJiWuxCiYjhwQfEy6SmhlFfU84oGgEfu?e=download\n",
            "Resolving doc-00-2o-docs.googleusercontent.com (doc-00-2o-docs.googleusercontent.com)... 172.217.31.129, 2404:6800:4004:808::2001\n",
            "Connecting to doc-00-2o-docs.googleusercontent.com (doc-00-2o-docs.googleusercontent.com)|172.217.31.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38680 (38K) [application/zip]\n",
            "Saving to: ‘src.zip’\n",
            "\n",
            "src.zip             100%[===================>]  37.77K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-05-12 20:14:17 (1.03 MB/s) - ‘src.zip’ saved [38680/38680]\n",
            "\n",
            "Archive:  src.zip\n",
            "replace src/main.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: src/main.py             \n",
            "  inflating: src/model.py            \n",
            "  inflating: src/configuration.py    \n",
            "  inflating: src/train.py            \n",
            "  inflating: src/run.py              \n",
            "  inflating: src/visualize.py        \n",
            "  inflating: src/configuration.yaml  \n",
            "  inflating: src/data_load.py        \n",
            "  inflating: src/test.mid            \n",
            "  inflating: src/data_augmentation.py  \n",
            "  inflating: src/feature_extraction.py  \n",
            "  inflating: src/utils.py            \n",
            "  inflating: src/__pycache__/data_load.cpython-36.pyc  \n",
            "  inflating: src/__pycache__/train.cpython-36.pyc  \n",
            "  inflating: src/__pycache__/utils.cpython-36.pyc  \n",
            "  inflating: src/__pycache__/feature_extraction.cpython-36.pyc  \n",
            "  inflating: src/__pycache__/data_augmentation.cpython-36.pyc  \n",
            "  inflating: src/__pycache__/model.cpython-36.pyc  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63KS-q53pKMp",
        "colab_type": "code",
        "outputId": "3a4bba79-2d6b-4ff8-8bf3-af4a117cf2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9649
        }
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1yIKscV2a_95_ymsordBYf0_qPOIKIw86' -O dataset.zip\n",
        "!unzip dataset.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-12 20:14:35--  https://docs.google.com/uc?export=download&id=1yIKscV2a_95_ymsordBYf0_qPOIKIw86\n",
            "Resolving docs.google.com (docs.google.com)... 216.58.197.206, 2404:6800:4004:80e::200e\n",
            "Connecting to docs.google.com (docs.google.com)|216.58.197.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-2o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jp45saugil9ao1kbk1f2qdc5a2bt21j7/1557691200000/10632806613498870968/*/1yIKscV2a_95_ymsordBYf0_qPOIKIw86?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-05-12 20:14:35--  https://doc-0g-2o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jp45saugil9ao1kbk1f2qdc5a2bt21j7/1557691200000/10632806613498870968/*/1yIKscV2a_95_ymsordBYf0_qPOIKIw86?e=download\n",
            "Resolving doc-0g-2o-docs.googleusercontent.com (doc-0g-2o-docs.googleusercontent.com)... 172.217.31.129, 2404:6800:4004:808::2001\n",
            "Connecting to doc-0g-2o-docs.googleusercontent.com (doc-0g-2o-docs.googleusercontent.com)|172.217.31.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip             [  <=>               ]   5.43M  14.8MB/s    in 0.4s    \n",
            "\n",
            "2019-05-12 20:14:36 (14.8 MB/s) - ‘dataset.zip’ saved [5692343]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "replace dataset/bach/bach344.mid? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: dataset/bach/bach344.mid  \n",
            "  inflating: dataset/bach/bach348.mid  \n",
            "  inflating: dataset/bach/bach354.mid  \n",
            "  inflating: dataset/bach/bach384.mid  \n",
            "  inflating: dataset/bach/bach349.mid  \n",
            "  inflating: dataset/bach/bach356.mid  \n",
            "  inflating: dataset/bach/bach359.mid  \n",
            "  inflating: dataset/bach/bach376.mid  \n",
            "  inflating: dataset/bach/.DS_Store  \n",
            "  inflating: dataset/bach/bach342.mid  \n",
            "  inflating: dataset/bach/bach343.mid  \n",
            "  inflating: dataset/bach/bach345.mid  \n",
            "  inflating: dataset/bach/bach346.mid  \n",
            "  inflating: dataset/bach/bach347.mid  \n",
            "  inflating: dataset/bach/bach350.mid  \n",
            "  inflating: dataset/bach/bach351.mid  \n",
            "  inflating: dataset/bach/bach352.mid  \n",
            "  inflating: dataset/bach/bach353.mid  \n",
            "  inflating: dataset/bach/bach355.mid  \n",
            "  inflating: dataset/bach/bach357.mid  \n",
            "  inflating: dataset/bach/bach358.mid  \n",
            "  inflating: dataset/bach/bach360.mid  \n",
            "  inflating: dataset/bach/bach361.mid  \n",
            "  inflating: dataset/bach/bach362.mid  \n",
            "  inflating: dataset/bach/bach363.mid  \n",
            "  inflating: dataset/bach/bach364.mid  \n",
            "  inflating: dataset/bach/bach365.mid  \n",
            "  inflating: dataset/bach/bach366.mid  \n",
            "  inflating: dataset/bach/bach367.mid  \n",
            "  inflating: dataset/bach/bach368.mid  \n",
            "  inflating: dataset/bach/bach369.mid  \n",
            "  inflating: dataset/bach/bach370.mid  \n",
            "  inflating: dataset/bach/bach371.mid  \n",
            "  inflating: dataset/bach/bach372.mid  \n",
            "  inflating: dataset/bach/bach373.mid  \n",
            "  inflating: dataset/bach/bach374.mid  \n",
            "  inflating: dataset/bach/bach375.mid  \n",
            "  inflating: dataset/bach/bach377.mid  \n",
            "  inflating: dataset/bach/bach378.mid  \n",
            "  inflating: dataset/bach/bach379.mid  \n",
            "  inflating: dataset/bach/bach380.mid  \n",
            "  inflating: dataset/bach/bach381.mid  \n",
            "  inflating: dataset/bach/bach382.mid  \n",
            "  inflating: dataset/bach/bach383.mid  \n",
            "  inflating: dataset/bach/bach385.mid  \n",
            "  inflating: dataset/bach/bach386.mid  \n",
            "  inflating: dataset/bach/bach387.mid  \n",
            "  inflating: dataset/bach/bach388.mid  \n",
            "  inflating: dataset/bach/bach389.mid  \n",
            "  inflating: dataset/bach/bach390.mid  \n",
            "  inflating: dataset/bach/bach391.mid  \n",
            "  inflating: dataset/bartok/bartok410.mid  \n",
            "  inflating: dataset/bartok/bartok412.mid  \n",
            "  inflating: dataset/bartok/bartok424.mid  \n",
            "  inflating: dataset/bartok/bartok434.mid  \n",
            "  inflating: dataset/bartok/bartok392.mid  \n",
            "  inflating: dataset/bartok/bartok400.mid  \n",
            "  inflating: dataset/bartok/bartok429.mid  \n",
            "  inflating: dataset/bartok/bartok438.mid  \n",
            "  inflating: dataset/bartok/.DS_Store  \n",
            "  inflating: dataset/bartok/bartok393.mid  \n",
            "  inflating: dataset/bartok/bartok394.mid  \n",
            "  inflating: dataset/bartok/bartok395.mid  \n",
            "  inflating: dataset/bartok/bartok396.mid  \n",
            "  inflating: dataset/bartok/bartok397.mid  \n",
            "  inflating: dataset/bartok/bartok398.mid  \n",
            "  inflating: dataset/bartok/bartok399.mid  \n",
            "  inflating: dataset/bartok/bartok401.mid  \n",
            "  inflating: dataset/bartok/bartok402.mid  \n",
            "  inflating: dataset/bartok/bartok403.mid  \n",
            "  inflating: dataset/bartok/bartok404.mid  \n",
            "  inflating: dataset/bartok/bartok405.mid  \n",
            "  inflating: dataset/bartok/bartok406.mid  \n",
            "  inflating: dataset/bartok/bartok407.mid  \n",
            "  inflating: dataset/bartok/bartok408.mid  \n",
            "  inflating: dataset/bartok/bartok409.mid  \n",
            "  inflating: dataset/bartok/bartok411.mid  \n",
            "  inflating: dataset/bartok/bartok413.mid  \n",
            "  inflating: dataset/bartok/bartok414.mid  \n",
            "  inflating: dataset/bartok/bartok415.mid  \n",
            "  inflating: dataset/bartok/bartok416.mid  \n",
            "  inflating: dataset/bartok/bartok417.mid  \n",
            "  inflating: dataset/bartok/bartok418.mid  \n",
            "  inflating: dataset/bartok/bartok419.mid  \n",
            "  inflating: dataset/bartok/bartok420.mid  \n",
            "  inflating: dataset/bartok/bartok421.mid  \n",
            "  inflating: dataset/bartok/bartok422.mid  \n",
            "  inflating: dataset/bartok/bartok423.mid  \n",
            "  inflating: dataset/bartok/bartok425.mid  \n",
            "  inflating: dataset/bartok/bartok426.mid  \n",
            "  inflating: dataset/bartok/bartok427.mid  \n",
            "  inflating: dataset/bartok/bartok428.mid  \n",
            "  inflating: dataset/bartok/bartok430.mid  \n",
            "  inflating: dataset/bartok/bartok431.mid  \n",
            "  inflating: dataset/bartok/bartok432.mid  \n",
            "  inflating: dataset/bartok/bartok433.mid  \n",
            "  inflating: dataset/bartok/bartok435.mid  \n",
            "  inflating: dataset/bartok/bartok436.mid  \n",
            "  inflating: dataset/bartok/bartok437.mid  \n",
            "  inflating: dataset/bartok/bartok439.mid  \n",
            "  inflating: dataset/bartok/bartok440.mid  \n",
            "  inflating: dataset/byrd/byrd152.mid  \n",
            "  inflating: dataset/byrd/byrd168.mid  \n",
            "  inflating: dataset/byrd/byrd180.mid  \n",
            "  inflating: dataset/byrd/byrd184.mid  \n",
            "  inflating: dataset/byrd/byrd150.mid  \n",
            "  inflating: dataset/byrd/byrd151.mid  \n",
            "  inflating: dataset/byrd/byrd174.mid  \n",
            "  inflating: dataset/byrd/byrd176.mid  \n",
            "  inflating: dataset/byrd/.DS_Store  \n",
            "  inflating: dataset/byrd/byrd149.mid  \n",
            "  inflating: dataset/byrd/byrd153.mid  \n",
            "  inflating: dataset/byrd/byrd154.mid  \n",
            "  inflating: dataset/byrd/byrd155.mid  \n",
            "  inflating: dataset/byrd/byrd156.mid  \n",
            "  inflating: dataset/byrd/byrd157.mid  \n",
            "  inflating: dataset/byrd/byrd158.mid  \n",
            "  inflating: dataset/byrd/byrd159.mid  \n",
            "  inflating: dataset/byrd/byrd160.mid  \n",
            "  inflating: dataset/byrd/byrd161.mid  \n",
            "  inflating: dataset/byrd/byrd162.mid  \n",
            "  inflating: dataset/byrd/byrd163.mid  \n",
            "  inflating: dataset/byrd/byrd164.mid  \n",
            "  inflating: dataset/byrd/byrd165.mid  \n",
            "  inflating: dataset/byrd/byrd166.mid  \n",
            "  inflating: dataset/byrd/byrd167.mid  \n",
            "  inflating: dataset/byrd/byrd169.mid  \n",
            "  inflating: dataset/byrd/byrd170.mid  \n",
            "  inflating: dataset/byrd/byrd171.mid  \n",
            "  inflating: dataset/byrd/byrd172.mid  \n",
            "  inflating: dataset/byrd/byrd173.mid  \n",
            "  inflating: dataset/byrd/byrd175.mid  \n",
            "  inflating: dataset/byrd/byrd177.mid  \n",
            "  inflating: dataset/byrd/byrd178.mid  \n",
            "  inflating: dataset/byrd/byrd179.mid  \n",
            "  inflating: dataset/byrd/byrd181.mid  \n",
            "  inflating: dataset/byrd/byrd182.mid  \n",
            "  inflating: dataset/byrd/byrd183.mid  \n",
            "  inflating: dataset/byrd/byrd185.mid  \n",
            "  inflating: dataset/byrd/byrd186.mid  \n",
            "  inflating: dataset/byrd/byrd187.mid  \n",
            "  inflating: dataset/byrd/byrd188.mid  \n",
            "  inflating: dataset/byrd/byrd189.mid  \n",
            "  inflating: dataset/byrd/byrd190.mid  \n",
            "  inflating: dataset/byrd/byrd191.mid  \n",
            "  inflating: dataset/byrd/byrd192.mid  \n",
            "  inflating: dataset/byrd/byrd193.mid  \n",
            "  inflating: dataset/byrd/byrd194.mid  \n",
            "  inflating: dataset/byrd/byrd195.mid  \n",
            "  inflating: dataset/byrd/byrd196.mid  \n",
            "  inflating: dataset/byrd/byrd197.mid  \n",
            "  inflating: dataset/byrd/byrd198.mid  \n",
            "  inflating: dataset/chopin/chopin061.mid  \n",
            "  inflating: dataset/chopin/chopin069.mid  \n",
            "  inflating: dataset/chopin/chopin070.mid  \n",
            "  inflating: dataset/chopin/chopin087.mid  \n",
            "  inflating: dataset/chopin/chopin053.mid  \n",
            "  inflating: dataset/chopin/chopin062.mid  \n",
            "  inflating: dataset/chopin/chopin078.mid  \n",
            "  inflating: dataset/chopin/chopin086.mid  \n",
            "  inflating: dataset/chopin/.DS_Store  \n",
            "  inflating: dataset/chopin/chopin049.mid  \n",
            "  inflating: dataset/chopin/chopin050.mid  \n",
            "  inflating: dataset/chopin/chopin051.mid  \n",
            "  inflating: dataset/chopin/chopin052.mid  \n",
            "  inflating: dataset/chopin/chopin054.mid  \n",
            "  inflating: dataset/chopin/chopin055.mid  \n",
            "  inflating: dataset/chopin/chopin056.mid  \n",
            "  inflating: dataset/chopin/chopin057.mid  \n",
            "  inflating: dataset/chopin/chopin058.mid  \n",
            "  inflating: dataset/chopin/chopin059.mid  \n",
            "  inflating: dataset/chopin/chopin060.mid  \n",
            "  inflating: dataset/chopin/chopin063.mid  \n",
            "  inflating: dataset/chopin/chopin064.mid  \n",
            "  inflating: dataset/chopin/chopin065.mid  \n",
            "  inflating: dataset/chopin/chopin067.mid  \n",
            "  inflating: dataset/chopin/chopin068.mid  \n",
            "  inflating: dataset/chopin/chopin071.mid  \n",
            "  inflating: dataset/chopin/chopin072.mid  \n",
            "  inflating: dataset/chopin/chopin073.mid  \n",
            "  inflating: dataset/chopin/chopin074.mid  \n",
            "  inflating: dataset/chopin/chopin075.mid  \n",
            "  inflating: dataset/chopin/chopin076.mid  \n",
            "  inflating: dataset/chopin/chopin077.mid  \n",
            "  inflating: dataset/chopin/chopin079.mid  \n",
            "  inflating: dataset/chopin/chopin080.mid  \n",
            "  inflating: dataset/chopin/chopin081.mid  \n",
            "  inflating: dataset/chopin/chopin082.mid  \n",
            "  inflating: dataset/chopin/chopin083.mid  \n",
            "  inflating: dataset/chopin/chopin084.mid  \n",
            "  inflating: dataset/chopin/chopin085.mid  \n",
            "  inflating: dataset/chopin/chopin088.mid  \n",
            "  inflating: dataset/chopin/chopin089.mid  \n",
            "  inflating: dataset/chopin/chopin090.mid  \n",
            "  inflating: dataset/chopin/chopin091.mid  \n",
            "  inflating: dataset/chopin/chopin092.mid  \n",
            "  inflating: dataset/chopin/chopin093.mid  \n",
            "  inflating: dataset/chopin/chopin094.mid  \n",
            "  inflating: dataset/chopin/chopin095.mid  \n",
            "  inflating: dataset/chopin/chopin096.mid  \n",
            "  inflating: dataset/chopin/chopin097.mid  \n",
            "  inflating: dataset/chopin/chopin098.mid  \n",
            "  inflating: dataset/handel/handel106.mid  \n",
            "  inflating: dataset/handel/handel119.mid  \n",
            "  inflating: dataset/handel/handel139.mid  \n",
            "  inflating: dataset/handel/handel145.mid  \n",
            "  inflating: dataset/handel/handel107.mid  \n",
            "  inflating: dataset/handel/handel112.mid  \n",
            "  inflating: dataset/handel/handel137.mid  \n",
            "  inflating: dataset/handel/handel147.mid  \n",
            "  inflating: dataset/handel/.DS_Store  \n",
            "  inflating: dataset/handel/handel099.mid  \n",
            "  inflating: dataset/handel/handel100.mid  \n",
            "  inflating: dataset/handel/handel101.mid  \n",
            "  inflating: dataset/handel/handel102.mid  \n",
            "  inflating: dataset/handel/handel103.mid  \n",
            "  inflating: dataset/handel/handel104.mid  \n",
            "  inflating: dataset/handel/handel105.mid  \n",
            "  inflating: dataset/handel/handel108.mid  \n",
            "  inflating: dataset/handel/handel109.mid  \n",
            "  inflating: dataset/handel/handel110.mid  \n",
            "  inflating: dataset/handel/handel111.mid  \n",
            "  inflating: dataset/handel/handel113.mid  \n",
            "  inflating: dataset/handel/handel114.mid  \n",
            "  inflating: dataset/handel/handel115.mid  \n",
            "  inflating: dataset/handel/handel116.mid  \n",
            "  inflating: dataset/handel/handel117.mid  \n",
            "  inflating: dataset/handel/handel118.mid  \n",
            "  inflating: dataset/handel/handel120.mid  \n",
            "  inflating: dataset/handel/handel121.mid  \n",
            "  inflating: dataset/handel/handel122.mid  \n",
            "  inflating: dataset/handel/handel123.mid  \n",
            "  inflating: dataset/handel/handel124.mid  \n",
            "  inflating: dataset/handel/handel125.mid  \n",
            "  inflating: dataset/handel/handel126.mid  \n",
            "  inflating: dataset/handel/handel127.mid  \n",
            "  inflating: dataset/handel/handel128.mid  \n",
            "  inflating: dataset/handel/handel129.mid  \n",
            "  inflating: dataset/handel/handel130.mid  \n",
            "  inflating: dataset/handel/handel131.mid  \n",
            "  inflating: dataset/handel/handel132.mid  \n",
            "  inflating: dataset/handel/handel133.mid  \n",
            "  inflating: dataset/handel/handel134.mid  \n",
            "  inflating: dataset/handel/handel135.mid  \n",
            "  inflating: dataset/handel/handel136.mid  \n",
            "  inflating: dataset/handel/handel138.mid  \n",
            "  inflating: dataset/handel/handel140.mid  \n",
            "  inflating: dataset/handel/handel141.mid  \n",
            "  inflating: dataset/handel/handel143.mid  \n",
            "  inflating: dataset/handel/handel144.mid  \n",
            "  inflating: dataset/handel/handel146.mid  \n",
            "  inflating: dataset/handel/handel148.mid  \n",
            "  inflating: dataset/hummel/hummel301.mid  \n",
            "  inflating: dataset/hummel/hummel303.mid  \n",
            "  inflating: dataset/hummel/hummel312.mid  \n",
            "  inflating: dataset/hummel/hummel327.mid  \n",
            "  inflating: dataset/hummel/hummel304.mid  \n",
            "  inflating: dataset/hummel/hummel314.mid  \n",
            "  inflating: dataset/hummel/hummel323.mid  \n",
            "  inflating: dataset/hummel/hummel335.mid  \n",
            "  inflating: dataset/hummel/.DS_Store  \n",
            "  inflating: dataset/hummel/hummel292.mid  \n",
            "  inflating: dataset/hummel/hummel293.mid  \n",
            "  inflating: dataset/hummel/hummel294.mid  \n",
            "  inflating: dataset/hummel/hummel295.mid  \n",
            "  inflating: dataset/hummel/hummel296.mid  \n",
            "  inflating: dataset/hummel/hummel297.mid  \n",
            "  inflating: dataset/hummel/hummel298.mid  \n",
            "  inflating: dataset/hummel/hummel299.mid  \n",
            "  inflating: dataset/hummel/hummel300.mid  \n",
            "  inflating: dataset/hummel/hummel302.mid  \n",
            "  inflating: dataset/hummel/hummel305.mid  \n",
            "  inflating: dataset/hummel/hummel306.mid  \n",
            "  inflating: dataset/hummel/hummel307.mid  \n",
            "  inflating: dataset/hummel/hummel308.mid  \n",
            "  inflating: dataset/hummel/hummel309.mid  \n",
            "  inflating: dataset/hummel/hummel310.mid  \n",
            "  inflating: dataset/hummel/hummel311.mid  \n",
            "  inflating: dataset/hummel/hummel313.mid  \n",
            "  inflating: dataset/hummel/hummel315.mid  \n",
            "  inflating: dataset/hummel/hummel316.mid  \n",
            "  inflating: dataset/hummel/hummel317.mid  \n",
            "  inflating: dataset/hummel/hummel318.mid  \n",
            "  inflating: dataset/hummel/hummel319.mid  \n",
            "  inflating: dataset/hummel/hummel320.mid  \n",
            "  inflating: dataset/hummel/hummel321.mid  \n",
            "  inflating: dataset/hummel/hummel322.mid  \n",
            "  inflating: dataset/hummel/hummel324.mid  \n",
            "  inflating: dataset/hummel/hummel325.mid  \n",
            "  inflating: dataset/hummel/hummel326.mid  \n",
            "  inflating: dataset/hummel/hummel328.mid  \n",
            "  inflating: dataset/hummel/hummel329.mid  \n",
            "  inflating: dataset/hummel/hummel330.mid  \n",
            "  inflating: dataset/hummel/hummel331.mid  \n",
            "  inflating: dataset/hummel/hummel332.mid  \n",
            "  inflating: dataset/hummel/hummel333.mid  \n",
            "  inflating: dataset/hummel/hummel334.mid  \n",
            "  inflating: dataset/hummel/hummel336.mid  \n",
            "  inflating: dataset/hummel/hummel337.mid  \n",
            "  inflating: dataset/hummel/hummel338.mid  \n",
            "  inflating: dataset/hummel/hummel339.mid  \n",
            "  inflating: dataset/hummel/hummel340.mid  \n",
            "  inflating: dataset/hummel/hummel341.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn258.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn262.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn265.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn270.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn259.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn282.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn284.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn288.mid  \n",
            "  inflating: dataset/mendelssohn/.DS_Store  \n",
            "  inflating: dataset/mendelssohn/mendelssohn243.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn244.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn245.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn246.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn247.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn248.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn249.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn250.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn251.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn252.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn253.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn254.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn255.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn256.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn257.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn260.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn261.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn263.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn264.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn266.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn267.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn268.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn269.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn271.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn272.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn273.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn274.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn275.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn276.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn277.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn278.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn279.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn280.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn281.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn283.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn285.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn286.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn287.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn289.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn290.mid  \n",
            "  inflating: dataset/mendelssohn/mendelssohn291.mid  \n",
            "  inflating: dataset/mozart/mozart020.mid  \n",
            "  inflating: dataset/mozart/mozart035.mid  \n",
            "  inflating: dataset/mozart/mozart039.mid  \n",
            "  inflating: dataset/mozart/mozart040.mid  \n",
            "  inflating: dataset/mozart/mozart004.mid  \n",
            "  inflating: dataset/mozart/mozart014.mid  \n",
            "  inflating: dataset/mozart/mozart025.mid  \n",
            "  inflating: dataset/mozart/mozart038.mid  \n",
            "  inflating: dataset/mozart/.DS_Store  \n",
            "  inflating: dataset/mozart/mozart000.mid  \n",
            "  inflating: dataset/mozart/mozart001.mid  \n",
            "  inflating: dataset/mozart/mozart002.mid  \n",
            "  inflating: dataset/mozart/mozart003.mid  \n",
            "  inflating: dataset/mozart/mozart005.mid  \n",
            "  inflating: dataset/mozart/mozart006.mid  \n",
            "  inflating: dataset/mozart/mozart007.mid  \n",
            "  inflating: dataset/mozart/mozart008.mid  \n",
            "  inflating: dataset/mozart/mozart009.mid  \n",
            "  inflating: dataset/mozart/mozart010.mid  \n",
            "  inflating: dataset/mozart/mozart011.mid  \n",
            "  inflating: dataset/mozart/mozart012.mid  \n",
            "  inflating: dataset/mozart/mozart013.mid  \n",
            "  inflating: dataset/mozart/mozart015.mid  \n",
            "  inflating: dataset/mozart/mozart016.mid  \n",
            "  inflating: dataset/mozart/mozart017.mid  \n",
            "  inflating: dataset/mozart/mozart018.mid  \n",
            "  inflating: dataset/mozart/mozart019.mid  \n",
            "  inflating: dataset/mozart/mozart021.mid  \n",
            "  inflating: dataset/mozart/mozart022.mid  \n",
            "  inflating: dataset/mozart/mozart023.mid  \n",
            "  inflating: dataset/mozart/mozart024.mid  \n",
            "  inflating: dataset/mozart/mozart026.mid  \n",
            "  inflating: dataset/mozart/mozart027.mid  \n",
            "  inflating: dataset/mozart/mozart028.mid  \n",
            "  inflating: dataset/mozart/mozart029.mid  \n",
            "  inflating: dataset/mozart/mozart030.mid  \n",
            "  inflating: dataset/mozart/mozart031.mid  \n",
            "  inflating: dataset/mozart/mozart032.mid  \n",
            "  inflating: dataset/mozart/mozart033.mid  \n",
            "  inflating: dataset/mozart/mozart034.mid  \n",
            "  inflating: dataset/mozart/mozart036.mid  \n",
            "  inflating: dataset/mozart/mozart037.mid  \n",
            "  inflating: dataset/mozart/mozart041.mid  \n",
            "  inflating: dataset/mozart/mozart042.mid  \n",
            "  inflating: dataset/mozart/mozart043.mid  \n",
            "  inflating: dataset/mozart/mozart044.mid  \n",
            "  inflating: dataset/mozart/mozart045.mid  \n",
            "  inflating: dataset/mozart/mozart046.mid  \n",
            "  inflating: dataset/mozart/mozart047.mid  \n",
            "  inflating: dataset/mozart/mozart048.mid  \n",
            "  inflating: dataset/schumann/schumann209.mid  \n",
            "  inflating: dataset/schumann/schumann217.mid  \n",
            "  inflating: dataset/schumann/schumann238.mid  \n",
            "  inflating: dataset/schumann/schumann205.mid  \n",
            "  inflating: dataset/schumann/schumann220.mid  \n",
            "  inflating: dataset/schumann/schumann240.mid  \n",
            "  inflating: dataset/schumann/.DS_Store  \n",
            "  inflating: dataset/schumann/schumann199.mid  \n",
            "  inflating: dataset/schumann/schumann200.mid  \n",
            "  inflating: dataset/schumann/schumann201.mid  \n",
            "  inflating: dataset/schumann/schumann202.mid  \n",
            "  inflating: dataset/schumann/schumann203.mid  \n",
            "  inflating: dataset/schumann/schumann204.mid  \n",
            "  inflating: dataset/schumann/schumann206.mid  \n",
            "  inflating: dataset/schumann/schumann207.mid  \n",
            "  inflating: dataset/schumann/schumann208.mid  \n",
            "  inflating: dataset/schumann/schumann210.mid  \n",
            "  inflating: dataset/schumann/schumann211.mid  \n",
            "  inflating: dataset/schumann/schumann212.mid  \n",
            "  inflating: dataset/schumann/schumann213.mid  \n",
            "  inflating: dataset/schumann/schumann214.mid  \n",
            "  inflating: dataset/schumann/schumann215.mid  \n",
            "  inflating: dataset/schumann/schumann216.mid  \n",
            "  inflating: dataset/schumann/schumann218.mid  \n",
            "  inflating: dataset/schumann/schumann219.mid  \n",
            "  inflating: dataset/schumann/schumann221.mid  \n",
            "  inflating: dataset/schumann/schumann222.mid  \n",
            "  inflating: dataset/schumann/schumann223.mid  \n",
            "  inflating: dataset/schumann/schumann224.mid  \n",
            "  inflating: dataset/schumann/schumann225.mid  \n",
            "  inflating: dataset/schumann/schumann226.mid  \n",
            "  inflating: dataset/schumann/schumann227.mid  \n",
            "  inflating: dataset/schumann/schumann228.mid  \n",
            "  inflating: dataset/schumann/schumann229.mid  \n",
            "  inflating: dataset/schumann/schumann230.mid  \n",
            "  inflating: dataset/schumann/schumann231.mid  \n",
            "  inflating: dataset/schumann/schumann232.mid  \n",
            "  inflating: dataset/schumann/schumann233.mid  \n",
            "  inflating: dataset/schumann/schumann234.mid  \n",
            "  inflating: dataset/schumann/schumann235.mid  \n",
            "  inflating: dataset/schumann/schumann236.mid  \n",
            "  inflating: dataset/schumann/schumann237.mid  \n",
            "  inflating: dataset/schumann/schumann239.mid  \n",
            "  inflating: dataset/schumann/schumann241.mid  \n",
            "  inflating: dataset/schumann/schumann242.mid  \n",
            "  inflating: dataset/other/clementi_opus36_1_1.mid  \n",
            "  inflating: dataset/other/clementi_opus36_1_2.mid  \n",
            "  inflating: dataset/other/clementi_opus36_1_3.mid  \n",
            "  inflating: dataset/other/clementi_opus36_2_1.mid  \n",
            "  inflating: dataset/other/clementi_opus36_2_2.mid  \n",
            "  inflating: dataset/other/clementi_opus36_2_3.mid  \n",
            "  inflating: dataset/other/clementi_opus36_3_1.mid  \n",
            "  inflating: dataset/other/clementi_opus36_3_2.mid  \n",
            "  inflating: dataset/other/clementi_opus36_3_3.mid  \n",
            "  inflating: dataset/other/clementi_opus36_4_1.mid  \n",
            "  inflating: dataset/other/clementi_opus36_4_2.mid  \n",
            "  inflating: dataset/other/clementi_opus36_4_3.mid  \n",
            "  inflating: dataset/other/clementi_opus36_5_1.mid  \n",
            "  inflating: dataset/other/clementi_opus36_5_2.mid  \n",
            "  inflating: dataset/other/clementi_opus36_5_3.mid  \n",
            "  inflating: dataset/other/clementi_opus36_6_1.mid  \n",
            "  inflating: dataset/other/clementi_opus36_6_2.mid  \n",
            "  inflating: dataset/other/hay_40_1.mid  \n",
            "  inflating: dataset/other/hay_40_2.mid  \n",
            "  inflating: dataset/other/haydn_7_1.mid  \n",
            "  inflating: dataset/other/haydn_7_2.mid  \n",
            "  inflating: dataset/other/haydn_7_3.mid  \n",
            "  inflating: dataset/other/haydn_8_1.mid  \n",
            "  inflating: dataset/other/haydn_8_2.mid  \n",
            "  inflating: dataset/other/haydn_8_3.mid  \n",
            "  inflating: dataset/other/haydn_8_4.mid  \n",
            "  inflating: dataset/other/haydn_9_1.mid  \n",
            "  inflating: dataset/other/haydn_9_2.mid  \n",
            "  inflating: dataset/other/haydn_9_3.mid  \n",
            "  inflating: dataset/other/haydn_33_1.mid  \n",
            "  inflating: dataset/other/haydn_33_2.mid  \n",
            "  inflating: dataset/other/haydn_33_3.mid  \n",
            "  inflating: dataset/other/haydn_35_1.mid  \n",
            "  inflating: dataset/other/haydn_35_2.mid  \n",
            "  inflating: dataset/other/haydn_35_3.mid  \n",
            "  inflating: dataset/other/haydn_43_1.mid  \n",
            "  inflating: dataset/other/haydn_43_2.mid  \n",
            "  inflating: dataset/other/haydn_43_3.mid  \n",
            "  inflating: dataset/other/appass_1.mid  \n",
            "  inflating: dataset/other/appass_2.mid  \n",
            "  inflating: dataset/other/appass_3.mid  \n",
            "  inflating: dataset/other/beethoven_hammerklavier_1.mid  \n",
            "  inflating: dataset/other/beethoven_hammerklavier_2.mid  \n",
            "  inflating: dataset/other/beethoven_hammerklavier_3.mid  \n",
            "  inflating: dataset/other/beethoven_hammerklavier_4.mid  \n",
            "  inflating: dataset/other/beethoven_les_adieux_1.mid  \n",
            "  inflating: dataset/other/beethoven_les_adieux_2.mid  \n",
            "  inflating: dataset/other/beethoven_les_adieux_3.mid  \n",
            "  inflating: dataset/other/beethoven_opus10_1.mid  \n",
            "  inflating: dataset/other/beethoven_opus10_2.mid  \n",
            "  inflating: dataset/other/beethoven_opus10_3.mid  \n",
            "  inflating: dataset/other/beethoven_opus22_1.mid  \n",
            "  inflating: dataset/other/beethoven_opus22_2.mid  \n",
            "  inflating: dataset/other/beethoven_opus22_3.mid  \n",
            "  inflating: dataset/other/beethoven_opus22_4.mid  \n",
            "  inflating: dataset/other/beethoven_opus90_1.mid  \n",
            "  inflating: dataset/other/beethoven_opus90_2.mid  \n",
            "  inflating: dataset/other/elise.mid  \n",
            "  inflating: dataset/other/mond_1.mid  \n",
            "  inflating: dataset/other/mond_2.mid  \n",
            "  inflating: dataset/other/mond_3.mid  \n",
            "  inflating: dataset/other/pathetique_1.mid  \n",
            "  inflating: dataset/other/pathetique_2.mid  \n",
            "  inflating: dataset/other/pathetique_3.mid  \n",
            "  inflating: dataset/other/waldstein_1.mid  \n",
            "  inflating: dataset/other/waldstein_2.mid  \n",
            "  inflating: dataset/other/waldstein_3.mid  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhOtV98jo5A5",
        "colab_type": "code",
        "outputId": "49510eaa-4599-4f88-a027-c3d40e7490ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = '1TUA7RZPQRzF4iisLqOhyN475xOtsOmHU' # URL id. \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('modelNone.h5')\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1TUA7RZPQRzF4iisLqOhyN475xOtsOmHU' -O modelNone.h5\n",
        "!mkdir out\n",
        "!mv modelNone.h5 out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 16.1MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 2.3MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 3.3MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 1.6MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 1.9MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 2.3MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 2.6MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 2.9MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 3.2MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 3.6MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 3.6MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 8.8MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 9.1MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 9.1MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 10.1MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 10.5MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 10.5MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 37.4MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 10.5MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 10.4MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 10.4MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 10.1MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 10.1MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 9.8MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 9.8MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 9.9MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 9.9MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 10.3MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 39.0MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 41.7MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 44.5MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 44.9MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 45.0MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 54.4MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 56.1MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 56.0MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 12.9MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 12.4MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 12.2MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 12.2MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 12.2MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 12.2MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 12.1MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 12.1MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 12.2MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 12.2MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 45.3MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 44.2MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 46.4MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 47.7MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 47.9MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 56.8MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 59.7MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 58.2MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 58.1MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 58.6MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 59.1MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 74.7MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 75.5MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 75.8MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 76.3MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 75.0MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 48.6MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 47.9MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 48.2MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 48.2MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 47.8MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 47.9MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 47.5MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 47.4MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 47.1MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 47.5MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 72.6MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 76.5MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 75.5MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 74.8MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 75.2MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 27.7MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 27.2MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 26.6MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 26.5MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 22.9MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 22.6MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 22.6MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 22.6MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 22.6MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 22.6MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 46.2MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 48.5MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 50.7MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 51.0MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 72.4MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 75.2MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 75.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 17.7MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&prompt=select_account&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline\n",
            "\n",
            "Enter verification code: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AuthorizationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthorizationError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-64add40f73e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Authenticate and create the PyDrive client.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# This only needs to be done once per notebook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_application_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclear_output\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m       \u001b[0m_gcloud_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0m_install_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mcolab_tpu_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36m_gcloud_login\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgcloud_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0m_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAuthorizationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error fetching credentials'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthorizationError\u001b[0m: Error fetching credentials"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QTZmkcS6Ec6",
        "colab_type": "code",
        "outputId": "b91e8c98-e1ec-4ac0-9a65-25eaac780af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5561
        }
      },
      "source": [
        "!cat src/model.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from tensorflow.keras.callbacks import ModelCheckpoint\n",
            "from tensorflow.keras.callbacks import EarlyStopping\n",
            "from tensorflow.keras.optimizers import Adam\n",
            "from tensorflow.keras.models import Model, Sequential\n",
            "from tensorflow.keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Dropout\n",
            "from tensorflow.keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
            "from tensorflow.keras.regularizers import l1, l2\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "import os\n",
            "\n",
            "def build_pure_cnn(conf):\n",
            "\tx0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
            "\tx = Conv2D(16, 3)(x0)\n",
            "\tx = BatchNormalization()(x)\n",
            "\tx = Activation('relu')(x)\n",
            "\tx = Conv2D(32, 3)(x)\n",
            "\tx = BatchNormalization()(x)\n",
            "\tx = Activation('relu')(x)\n",
            "\tx = MaxPooling2D()(x)\n",
            "\tx = Conv2D(64, 3)(x)\n",
            "\tx = BatchNormalization()(x)\n",
            "\tx = Activation('relu')(x)\n",
            "\tx = Conv2D(128, 3)(x)\n",
            "\tx = BatchNormalization()(x)\n",
            "\tx = Activation('relu')(x)\n",
            "\tx = MaxPooling2D()(x)\n",
            "\tx = Conv2D(256, 3)(x)\n",
            "\tx = BatchNormalization()(x)\n",
            "\tx = Activation('relu')(x)\n",
            "\tx = Conv2D(512, 3)(x)\n",
            "\tx = BatchNormalization()(x)\n",
            "\tx = Activation('relu')(x)\n",
            "\tx = MaxPooling2D()(x)\n",
            "\tx = Flatten()(x)\n",
            "\tx = Dense(128)(x)\n",
            "\tx = BatchNormalization()(x)\n",
            "\tx = Activation('relu')(x)\n",
            "\tx = Dense(conf['dataset']['num_class'])(x)\n",
            "\tx = Activation('softmax')(x)\n",
            "\tmodel = Model(inputs = x0, outputs = x)\n",
            "\treturn model\n",
            "\n",
            "def build_pure_cnn_drop(conf):\n",
            "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
            "  x = Conv2D(16, 3)(x0)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(32, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(64, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(128, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(256, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(512, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Flatten()(x)\n",
            "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
            "  x = Dense(128)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
            "  x = Dense(conf['dataset']['num_class'])(x)\n",
            "  x = Activation('softmax')(x)\n",
            "  model = Model(inputs = x0, outputs = x)\n",
            "  return model\n",
            "\n",
            "def build_moderate_cnn_drop(conf):\n",
            "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
            "  x = Conv2D(16, 3)(x0)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
            "  x = Conv2D(32, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(64, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
            "  x = Conv2D(128, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(256, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Flatten()(x)\n",
            "  x = Dense(128)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
            "  x = Dense(conf['dataset']['num_class'])(x)\n",
            "  x = Activation('softmax')(x)\n",
            "  model = Model(inputs = x0, outputs = x)\n",
            "  return model\n",
            "\n",
            "def build_moderate_cnn_l2(conf):\n",
            "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
            "  x = Conv2D(16, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x0)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(32, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(64, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(128, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(256, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Flatten()(x)\n",
            "  x = Dense(128)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dense(conf['dataset']['num_class'])(x)\n",
            "  x = Activation('softmax')(x)\n",
            "  model = Model(inputs = x0, outputs = x)\n",
            "  return model\n",
            "\n",
            "def build_moderate_cnn(conf):\n",
            "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
            "  x = Conv2D(16, 5)(x0)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(32, 5)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(64, 5)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(128, 5)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(256, 5)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Flatten()(x)\n",
            "  x = Dense(128)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dense(conf['dataset']['num_class'])(x)\n",
            "  x = Activation('softmax')(x)\n",
            "  model = Model(inputs = x0, outputs = x)\n",
            "  return model\n",
            "\n",
            "def build_big_cnn(conf):\n",
            "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
            "  x = Conv2D(16, 3)(x0)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(32, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(64, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(128, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(256, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Conv2D(512, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(1024, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Conv2D(2048, 3)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = MaxPooling2D()(x)\n",
            "  x = Flatten()(x)\n",
            "  x = Dense(128)(x)\n",
            "  x = BatchNormalization()(x)\n",
            "  x = Activation('relu')(x)\n",
            "  x = Dense(conf['dataset']['num_class'])(x)\n",
            "  x = Activation('softmax')(x)\n",
            "  model = Model(inputs = x0, outputs = x)\n",
            "  return model\n",
            "\n",
            "def build_rnn(conf):\n",
            "\treturn None\n",
            "\n",
            "def build_dummy(conf):\n",
            "\tx0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
            "\tx = Flatten()(x0)\n",
            "\tx = Dense(conf['dataset']['num_class'])(x)\n",
            "\tx = Activation('softmax')(x)\n",
            "\tmodel = Model(inputs = x0, outputs = x)\n",
            "\treturn model\n",
            "\n",
            "def list_models_methods():\n",
            "\tmodel_builders = {}\n",
            "\tmodel_builders['cnn'] = build_pure_cnn\n",
            "\tmodel_builders['cnn_drop'] = build_pure_cnn_drop\n",
            "\tmodel_builders['mod_cnn'] = build_moderate_cnn\n",
            "\tmodel_builders['mod_cnn_l2'] = build_moderate_cnn_l2\n",
            "\tmodel_builders['mod_cnn_drop'] = build_moderate_cnn_drop\n",
            "\tmodel_builders['big_cnn'] = build_big_cnn\n",
            "\tmodel_builders['rnn'] = build_rnn\n",
            "\tmodel_builders['None'] = build_dummy\n",
            "\treturn model_builders\n",
            "\n",
            "def get_model_path(conf):\n",
            "\treturn conf['model']['save_path'] + conf['model']['type'] + '.h5'\n",
            "\n",
            "def build_model(conf):\n",
            "\tos.makedirs(os.path.dirname(get_model_path(conf)), exist_ok=True)\n",
            "\tlist_model = list_models_methods()\n",
            "\tmodel_name = conf['model']['type']\n",
            "\tif (model_name in list_model):\n",
            "\t\tmodel = list_model[model_name](conf)\n",
            "\t\tprint(model_name, ' model has been built')\n",
            "\telse:\n",
            "\t\tprint(model_name, ' is invalid')\n",
            "\t\tprint('Available models are ', list_model.keys())\n",
            "\t\texit()\n",
            "\tparameters = conf['model']['parameters']\n",
            "\topt = tf.train.AdamOptimizer(learning_rate=parameters['learning_rate'], beta1=parameters['beta_1'], beta2=parameters['beta_2'])\n",
            "\tmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
            "\tmodel.summary()\n",
            "\tif (conf['model']['tpu']):\n",
            "\t\ttry:\n",
            "\t\t    device_name = os.environ['COLAB_TPU_ADDR']\n",
            "\t\t    TPU_ADDRESS = 'grpc://' + device_name\n",
            "\t\t    print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
            "\t\texcept KeyError:\n",
            "\t\t    print('TPU not found')\n",
            "\t\tmodel = tf.contrib.tpu.keras_to_tpu_model(\n",
            "\t\t    model,\n",
            "\t\t    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
            "\t\t        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
            "\ttry:\n",
            "\t\tmodel.load_weights(get_model_path(conf))\n",
            "\t\tprint('Loaded model from file.')\n",
            "\texcept:\n",
            "\t\tprint('Unable to load model from file.')\n",
            "\treturn model\n",
            "def train_model(conf, train_x, train_y, val_x, val_y, model):\n",
            "\tparameters = conf['model']['parameters']\n",
            "\tcbs = [\n",
            "\t\tModelCheckpoint(get_model_path(conf), monitor='val_loss', save_best_only=True, save_weights_only=True),\n",
            "\t\tEarlyStopping(monitor='val_loss', patience=5)\n",
            "\t]\n",
            "\tif (len(train_x) == 1):\n",
            "\t\thistory = model.fit(train_x[0], train_y, epochs=parameters['epochs'], validation_data=(val_x[0], val_y), callbacks=cbs, batch_size=parameters['batch_size'], shuffle = True)\n",
            "\telse:\n",
            "\t\thistory = model.fit(train_x, train_y, epochs=parameters['epochs'], validation_data=(val_x, val_y), callbacks=cbs, batch_size=parameters['batch_size'], shuffle = True)\n",
            "\t# Plot training & validation accuracy values\n",
            "\tplt.plot(history.history['acc'])\n",
            "\tplt.plot(history.history['val_acc'])\n",
            "\tplt.title('Model accuracy')\n",
            "\tplt.ylabel('Accuracy')\n",
            "\tplt.xlabel('Epoch')\n",
            "\tplt.legend(['Train', 'Test'], loc='upper left')\n",
            "\tplt.savefig(conf['model']['type'] + '_train_val_acc.png')\n",
            "\tplt.show()\n",
            "\t# Plot training & validation loss values\n",
            "\tplt.plot(history.history['loss'])\n",
            "\tplt.plot(history.history['val_loss'])\n",
            "\tplt.title('Model loss')\n",
            "\tplt.ylabel('Loss')\n",
            "\tplt.xlabel('Epoch')\n",
            "\tplt.legend(['Train', 'Test'], loc='upper left')\n",
            "\tplt.savefig(conf['model']['type'] + '_train_val_loss.png')\n",
            "\tplt.show()\n",
            "def evaluate_model(conf, model, test_X, test_y):\n",
            "\tif (len(test_X) == 1):\n",
            "\t\ttest_loss, test_acc = model.evaluate(test_X[0], test_y, verbose = 0, batch_size = conf['model']['parameters']['batch_size'])\n",
            "\telse:\n",
            "\t\ttest_loss, test_acc = model.evaluate(test_X, test_y, verbose = 0, batch_size = conf['model']['parameters']['batch_size'])\n",
            "\tprint('Test loss:', test_loss)\n",
            "\tprint('Test accuracy:', test_acc)\n",
            "def predict_model(conf, model, x):\n",
            "\tif (len(x) == 1):\n",
            "\t\treturn model.predict(x[0], batch_size = conf['model']['parameters']['batch_size'])\n",
            "\telse:\n",
            "\t\treturn model.predict(x, batch_size = conf['model']['parameters']['batch_size'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmBu-lsJ6H-4",
        "colab_type": "code",
        "outputId": "a7957a0f-dff7-4b5c-e452-2d10e6c07fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile src/model.py\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Dropout, LSTM\n",
        "from tensorflow.keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, Permute\n",
        "from tensorflow.keras.layers import Reshape, Conv1D, MaxPooling1D, GRU, TimeDistributed\n",
        "from keras.utils import plot_model\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "def build_pure_cnn(conf):\n",
        "    x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "    x = Conv2D(16, 3)(x0)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32, 3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Conv2D(64, 3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(128, 3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Conv2D(256, 3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(512, 3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(conf['dataset']['num_class'])(x)\n",
        "    x = Activation('softmax')(x)\n",
        "    model = Model(inputs = x0, outputs = x)\n",
        "    return model\n",
        "\n",
        "def build_pure_cnn_drop(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Conv2D(16, 3)(x0)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(32, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(64, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(128, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(256, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(512, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_moderate_cnn_drop(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Conv2D(16, 3)(x0)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = Conv2D(32, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(64, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = Conv2D(128, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(256, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_moderate_cnn_l2(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Conv2D(16, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x0)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(32, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(64, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(128, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(256, 3, activity_regularizer=l2(conf['model']['parameters']['l2']))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_moderate_cnn(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Conv2D(16, 5)(x0)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(32, 5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(64, 5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(128, 5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(256, 5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_big_cnn(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Conv2D(16, 3)(x0)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(32, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(64, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(128, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(256, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(512, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(1024, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Conv2D(2048, 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D()(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = LSTM(128, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers1(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = LSTM(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(256)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers11(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = GRU(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(256)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers11_Dropout(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = GRU(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(256)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = LSTM(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(256)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(64)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers2(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = LSTM(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(512)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(64)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers22(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = GRU(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(512)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(64)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers3(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = LSTM(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(512)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(256)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers33(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = GRU(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(512)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(256)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers4(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = GRU(1024, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "def build_rnn_dense_layers5(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = GRU(256, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(conf['model']['parameters']['dropout'])(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model\n",
        "\n",
        "\n",
        "def build_rnn_two_lstm(conf):\n",
        "  x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "  x = Reshape((conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size']))(x0)\n",
        "  x = Permute((2, 1))(x)\n",
        "  x = LSTM(128, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'], return_sequences=True)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LSTM(64, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout'])(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(conf['dataset']['num_class'])(x)\n",
        "  x = Activation('softmax')(x)\n",
        "  model = Model(inputs = x0, outputs = x)\n",
        "  return model  \n",
        "def build_hybrid(conf):\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Conv1D(16, 5, activity_regularizer=l2(conf['model']['parameters']['l2'])))\n",
        "    cnn_model.add(BatchNormalization())\n",
        "    cnn_model.add(Activation('relu'))\n",
        "    cnn_model.add(Conv1D(32, 5, activity_regularizer=l2(conf['model']['parameters']['l2'])))\n",
        "    cnn_model.add(BatchNormalization())\n",
        "    cnn_model.add(Activation('relu'))\n",
        "    cnn_model.add(MaxPooling1D())\n",
        "    cnn_model.add(Conv1D(64, 5, activity_regularizer=l2(conf['model']['parameters']['l2'])))\n",
        "    cnn_model.add(BatchNormalization())\n",
        "    cnn_model.add(Activation('relu'))\n",
        "    cnn_model.add(MaxPooling1D())\n",
        "    cnn_model.add(Flatten())\n",
        "    rnn_model = Sequential()\n",
        "    rnn_model.add(Permute((2,1,3), input_shape = (conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1)))\n",
        "    rnn_model.add(TimeDistributed(cnn_model))\n",
        "    rnn_model.add(GRU(128, dropout=conf['model']['parameters']['dropout'], recurrent_dropout=conf['model']['parameters']['dropout']))\n",
        "    rnn_model.add(Dense(conf['dataset']['num_class']))\n",
        "    rnn_model.add(Activation('softmax'))\n",
        "    return rnn_model\n",
        "\n",
        "def build_dummy(conf):\n",
        "    x0 = Input(shape=(conf['feature_extraction']['max_note'] - conf['feature_extraction']['min_note'] + 1, conf['data_augmentation']['sample_size'], 1))\n",
        "    x = Flatten()(x0)\n",
        "    x = Dense(conf['dataset']['num_class'])(x)\n",
        "    x = Activation('softmax')(x)\n",
        "    model = Model(inputs = x0, outputs = x)\n",
        "    return model\n",
        "\n",
        "def list_models_methods():\n",
        "  model_builders = {}\n",
        "  model_builders['cnn'] = build_pure_cnn\n",
        "  model_builders['cnn_drop'] = build_pure_cnn_drop\n",
        "  model_builders['mod_cnn'] = build_moderate_cnn\n",
        "  model_builders['mod_cnn_l2'] = build_moderate_cnn_l2\n",
        "  model_builders['mod_cnn_drop'] = build_moderate_cnn_drop\n",
        "  model_builders['big_cnn'] = build_big_cnn\n",
        "  model_builders['rnn_two_lstm'] = build_rnn_two_lstm\n",
        "  model_builders['rnn_dense_layers'] = build_rnn_dense_layers\n",
        "  model_builders['rnn_dense_layers1'] = build_rnn_dense_layers1\n",
        "  model_builders['rnn_dense_layers2'] = build_rnn_dense_layers2\n",
        "  model_builders['rnn_dense_layers3'] = build_rnn_dense_layers3\n",
        "  model_builders['rnn_dense_layers4'] = build_rnn_dense_layers4\n",
        "  model_builders['rnn_dense_layers33'] = build_rnn_dense_layers33\n",
        "  model_builders['rnn_dense_layers22'] = build_rnn_dense_layers22\n",
        "  model_builders['rnn_dense_layers11'] = build_rnn_dense_layers11\n",
        "  model_builders['rnn_dense_layers5'] = build_rnn_dense_layers5\n",
        "  model_builders['rnn_dense_layers11_Dropout'] = build_rnn_dense_layers11_Dropout\n",
        "  model_builders['rnn'] = build_rnn\n",
        "  model_builders['hybrid'] = build_hybrid\n",
        "  model_builders['None'] = build_dummy\n",
        "  return model_builders\n",
        "\n",
        "\n",
        "\n",
        "def get_model_path(conf):\n",
        "    return conf['model']['save_path'] + conf['model']['type'] + '.h5'\n",
        "\n",
        "def build_model(conf):\n",
        "    os.makedirs(os.path.dirname(get_model_path(conf)), exist_ok=True)\n",
        "    list_model = list_models_methods()\n",
        "    model_name = conf['model']['type']\n",
        "    if (model_name in list_model):\n",
        "        model = list_model[model_name](conf)\n",
        "        print(model_name, ' model has been built')\n",
        "    else:\n",
        "        print(model_name, ' is invalid')\n",
        "        print('Available models are ', list_model.keys())\n",
        "        exit()\n",
        "    parameters = conf['model']['parameters']\n",
        "    opt = tf.train.AdamOptimizer(learning_rate=parameters['learning_rate'], beta1=parameters['beta_1'], beta2=parameters['beta_2'])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png')\n",
        "    if (conf['model']['tpu']):\n",
        "        try:\n",
        "            device_name = os.environ['COLAB_TPU_ADDR']\n",
        "            TPU_ADDRESS = 'grpc://' + device_name\n",
        "            print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "        except KeyError:\n",
        "            print('TPU not found')\n",
        "        model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "            model,\n",
        "            strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
        "    try:\n",
        "        model.load_weights(get_model_path(conf))\n",
        "        print('Loaded model from file.')\n",
        "    except:\n",
        "        print('Unable to load model from file.')\n",
        "    return model\n",
        "def train_model(conf, train_x, train_y, val_x, val_y, model):\n",
        "    parameters = conf['model']['parameters']\n",
        "    cbs = [\n",
        "        ModelCheckpoint(get_model_path(conf), monitor='val_loss', save_best_only=True, save_weights_only=True),\n",
        "        EarlyStopping(monitor='val_loss', patience=10)\n",
        "    ]\n",
        "    if (len(train_x) == 1):\n",
        "        history = model.fit(train_x[0], train_y, epochs=parameters['epochs'], validation_data=(val_x[0], val_y), callbacks=cbs, batch_size=parameters['batch_size'], shuffle = True)\n",
        "    else:\n",
        "        history = model.fit(train_x, train_y, epochs=parameters['epochs'], validation_data=(val_x, val_y), callbacks=cbs, batch_size=parameters['batch_size'], shuffle = True)\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.savefig(conf['model']['type'] + '_train_val_acc.png')\n",
        "    plt.show()\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.savefig(conf['model']['type'] + '_train_val_loss.png')\n",
        "    plt.show()\n",
        "def evaluate_model(conf, model, test_X, test_y):\n",
        "    if (len(test_X) == 1):\n",
        "        test_loss, test_acc = model.evaluate(test_X[0], test_y, verbose = 0, batch_size = conf['model']['parameters']['batch_size'])\n",
        "    else:\n",
        "        test_loss, test_acc = model.evaluate(test_X, test_y, verbose = 0, batch_size = conf['model']['parameters']['batch_size'])\n",
        "    print('Test loss:', test_loss)\n",
        "    print('Test accuracy:', test_acc)\n",
        "def predict_model(conf, model, x):\n",
        "    if (len(x) == 1):\n",
        "        return model.predict(x[0], batch_size = conf['model']['parameters']['batch_size'])\n",
        "    else:\n",
        "        return model.predict(x, batch_size = conf['model']['parameters']['batch_size'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting src/model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMa-FWfqrwpi",
        "colab_type": "code",
        "outputId": "a1f8d467-218c-487e-afab-6b802920b47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile config.yaml\n",
        "model:\n",
        "    type: hybrid\n",
        "    tpu: no\n",
        "    save_path: out/model\n",
        "    parameters:\n",
        "          batch_size: 128\n",
        "          epochs: 100\n",
        "          learning_rate: 0.00009\n",
        "          beta_1: 0.9\n",
        "          beta_2: 0.999\n",
        "          dropout: 0.5\n",
        "          l2: 1\n",
        "dataset:\n",
        "    raw_path: dataset\n",
        "    num_class: 10\n",
        "feature_extraction:\n",
        "    min_note: 0\n",
        "    max_note: 127\n",
        "    features:\n",
        "data_augmentation:\n",
        "    sample_size: 300\n",
        "    num_samples_per_track: 60"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6TdpRqUpXEJ",
        "colab_type": "code",
        "outputId": "07f7b954-d749-4966-b740-06fa0e6d0465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3259
        }
      },
      "source": [
        "# Train\n",
        "!python3 src/main.py -t -c config.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "-------------------------------------------------------------------------------------\n",
            "Configuration: \n",
            "\tmodel:\n",
            "\t\ttype:\n",
            "\t\t\thybrid\n",
            "\t\ttpu:\n",
            "\t\t\tFalse\n",
            "\t\tsave_path:\n",
            "\t\t\tout/model\n",
            "\t\tparameters:\n",
            "\t\t\tbatch_size:\n",
            "\t\t\t\t128\n",
            "\t\t\tepochs:\n",
            "\t\t\t\t100\n",
            "\t\t\tlearning_rate:\n",
            "\t\t\t\t9e-05\n",
            "\t\t\tbeta_1:\n",
            "\t\t\t\t0.9\n",
            "\t\t\tbeta_2:\n",
            "\t\t\t\t0.999\n",
            "\t\t\tdropout:\n",
            "\t\t\t\t0.5\n",
            "\t\t\tl2:\n",
            "\t\t\t\t1\n",
            "\tdataset:\n",
            "\t\traw_path:\n",
            "\t\t\tdataset\n",
            "\t\tnum_class:\n",
            "\t\t\t10\n",
            "\tfeature_extraction:\n",
            "\t\tmin_note:\n",
            "\t\t\t0\n",
            "\t\tmax_note:\n",
            "\t\t\t127\n",
            "\t\tfeatures:\n",
            "\t\t\tNone\n",
            "\tdata_augmentation:\n",
            "\t\tsample_size:\n",
            "\t\t\t300\n",
            "\t\tnum_samples_per_track:\n",
            "\t\t\t60\n",
            "-------------------------------------------------------------------------------------\n",
            "Reading raw data...\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "Midi files loaded : 506\n",
            "Different classes :  ['mozart', 'bach', 'other', 'handel', 'byrd', 'hummel', 'mendelssohn', 'chopin', 'schumann', 'bartok']\n",
            "Filter raw data...\n",
            "Preprocessing composer  mozart  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  bach  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  other  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  handel  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  byrd  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  hummel  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  mendelssohn  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  chopin  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  schumann  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  bartok  data\n",
            "pianorolls feature has been constructed\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "hybrid  model has been built\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute (Permute)            (None, 300, 128, 1)       0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 300, 1792)         13440     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 128)               737664    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 752,394\n",
            "Trainable params: 752,170\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n",
            "2019-05-12 22:36:08.788098: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-05-12 22:36:08.788301: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x69fea160 executing computations on platform Host. Devices:\n",
            "2019-05-12 22:36:08.788331: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-12 22:36:08.949056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-12 22:36:08.949968: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x69fea420 executing computations on platform CUDA. Devices:\n",
            "2019-05-12 22:36:08.950002: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-12 22:36:08.950388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-12 22:36:08.950414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-12 22:36:09.337431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-12 22:36:09.337506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-12 22:36:09.337518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-12 22:36:09.337767: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-12 22:36:09.337808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Loaded model from file.\n",
            "tcmalloc: large alloc 6486220800 bytes == 0x7f1fb5642000 @  0x7f21e4e921e7 0x7f21e2a5bca1 0x7f21e2ac0718 0x7f21e2ac0833 0x7f21e2b60b2e 0x7f21e2b61394 0x7f21e2b614e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f21e4a8fb97 0x5afa0a\n",
            "tcmalloc: large alloc 1290240000 bytes == 0x7f1f687ca000 @  0x7f21e4e921e7 0x7f21e2a5bca1 0x7f21e2ac0718 0x7f21e2ac0833 0x7f21e2b60b2e 0x7f21e2b61394 0x7f21e2b614e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f21e4a8fb97 0x5afa0a\n",
            "Train on 21114 samples, validate on 4200 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "2019-05-12 22:36:15.866272: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "21114/21114 [==============================] - 210s 10ms/sample - loss: 0.3781 - acc: 0.8782 - val_loss: 0.8585 - val_acc: 0.7086\n",
            "Epoch 2/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.3572 - acc: 0.8882 - val_loss: 0.8474 - val_acc: 0.7250\n",
            "Epoch 3/100\n",
            "21114/21114 [==============================] - 200s 9ms/sample - loss: 0.3344 - acc: 0.8932 - val_loss: 0.8268 - val_acc: 0.7195\n",
            "Epoch 4/100\n",
            "21114/21114 [==============================] - 198s 9ms/sample - loss: 0.3275 - acc: 0.8963 - val_loss: 0.8647 - val_acc: 0.7207\n",
            "Epoch 5/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.3097 - acc: 0.9033 - val_loss: 0.8278 - val_acc: 0.7252\n",
            "Epoch 6/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2924 - acc: 0.9058 - val_loss: 0.8396 - val_acc: 0.7179\n",
            "Epoch 7/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2807 - acc: 0.9129 - val_loss: 0.8149 - val_acc: 0.7279\n",
            "Epoch 8/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2755 - acc: 0.9133 - val_loss: 0.8462 - val_acc: 0.7224\n",
            "Epoch 9/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2628 - acc: 0.9187 - val_loss: 0.8314 - val_acc: 0.7293\n",
            "Epoch 10/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2542 - acc: 0.9219 - val_loss: 0.8237 - val_acc: 0.7210\n",
            "Epoch 11/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2431 - acc: 0.9246 - val_loss: 0.8086 - val_acc: 0.7329\n",
            "Epoch 12/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2370 - acc: 0.9266 - val_loss: 0.8285 - val_acc: 0.7257\n",
            "Epoch 13/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2281 - acc: 0.9295 - val_loss: 0.8205 - val_acc: 0.7324\n",
            "Epoch 14/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2200 - acc: 0.9312 - val_loss: 0.8319 - val_acc: 0.7317\n",
            "Epoch 15/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.2098 - acc: 0.9364 - val_loss: 0.8280 - val_acc: 0.7236\n",
            "Epoch 16/100\n",
            "21114/21114 [==============================] - 200s 9ms/sample - loss: 0.2066 - acc: 0.9359 - val_loss: 0.8392 - val_acc: 0.7271\n",
            "Epoch 17/100\n",
            "21114/21114 [==============================] - 198s 9ms/sample - loss: 0.1972 - acc: 0.9407 - val_loss: 0.8610 - val_acc: 0.7395\n",
            "Epoch 18/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.1914 - acc: 0.9423 - val_loss: 0.8245 - val_acc: 0.7362\n",
            "Epoch 19/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.1856 - acc: 0.9434 - val_loss: 0.8393 - val_acc: 0.7395\n",
            "Epoch 20/100\n",
            "21114/21114 [==============================] - 199s 9ms/sample - loss: 0.1814 - acc: 0.9459 - val_loss: 0.8429 - val_acc: 0.7317\n",
            "Epoch 21/100\n",
            "21114/21114 [==============================] - 198s 9ms/sample - loss: 0.1756 - acc: 0.9465 - val_loss: 0.8633 - val_acc: 0.7393\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j0wapjIiN3c",
        "colab_type": "code",
        "outputId": "1bd19604-db2c-4e84-ffe6-b0db36ea2f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2665
        }
      },
      "source": [
        "# Evaluate\n",
        "!python3 src/main.py -e -c config.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "-------------------------------------------------------------------------------------\n",
            "Configuration: \n",
            "\tmodel:\n",
            "\t\ttype:\n",
            "\t\t\thybrid\n",
            "\t\ttpu:\n",
            "\t\t\tFalse\n",
            "\t\tsave_path:\n",
            "\t\t\tout/model\n",
            "\t\tparameters:\n",
            "\t\t\tbatch_size:\n",
            "\t\t\t\t128\n",
            "\t\t\tepochs:\n",
            "\t\t\t\t100\n",
            "\t\t\tlearning_rate:\n",
            "\t\t\t\t0.0001\n",
            "\t\t\tbeta_1:\n",
            "\t\t\t\t0.9\n",
            "\t\t\tbeta_2:\n",
            "\t\t\t\t0.999\n",
            "\t\t\tdropout:\n",
            "\t\t\t\t0.3\n",
            "\t\t\tl2:\n",
            "\t\t\t\t0.002\n",
            "\tdataset:\n",
            "\t\traw_path:\n",
            "\t\t\tdataset\n",
            "\t\tnum_class:\n",
            "\t\t\t10\n",
            "\tfeature_extraction:\n",
            "\t\tmin_note:\n",
            "\t\t\t0\n",
            "\t\tmax_note:\n",
            "\t\t\t127\n",
            "\t\tfeatures:\n",
            "\t\t\tNone\n",
            "\tdata_augmentation:\n",
            "\t\tsample_size:\n",
            "\t\t\t300\n",
            "\t\tnum_samples_per_track:\n",
            "\t\t\t60\n",
            "-------------------------------------------------------------------------------------\n",
            "Reading raw data...\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "Midi files loaded : 506\n",
            "Different classes :  ['mozart', 'bach', 'other', 'handel', 'byrd', 'hummel', 'mendelssohn', 'chopin', 'schumann', 'bartok']\n",
            "Filter raw data...\n",
            "Preprocessing composer  mozart  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  bach  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  other  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  handel  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  byrd  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  hummel  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  mendelssohn  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  chopin  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  schumann  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  bartok  data\n",
            "pianorolls feature has been constructed\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "hybrid  model has been built\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute (Permute)            (None, 300, 128, 1)       0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 300, 1792)         13440     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 128)               737664    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 752,394\n",
            "Trainable params: 752,170\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n",
            "2019-05-12 22:09:34.000855: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-05-12 22:09:34.001136: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1c56c160 executing computations on platform Host. Devices:\n",
            "2019-05-12 22:09:34.001170: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-12 22:09:34.167738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-12 22:09:34.168348: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1c56c420 executing computations on platform CUDA. Devices:\n",
            "2019-05-12 22:09:34.168380: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-12 22:09:34.168750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-12 22:09:34.168787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-12 22:09:34.553554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-12 22:09:34.553617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-12 22:09:34.553629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-12 22:09:34.553884: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-12 22:09:34.553926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Loaded model from file.\n",
            "tcmalloc: large alloc 1489616896 bytes == 0xa6786000 @  0x7f371e21f1e7 0x7f371bde8ca1 0x7f371be4d718 0x7f371be4d833 0x7f371beedb2e 0x7f371beee394 0x7f371beee4e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f371de1cb97 0x5afa0a\n",
            "Single sample test results : \n",
            "2019-05-12 22:09:35.658522: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "Test loss: 0.8626234033447513\n",
            "Test accuracy: 0.731491\n",
            "Full track test results : \n",
            "tcmalloc: large alloc 4924727296 bytes == 0x7f345276a000 @  0x7f371e21f1e7 0x7f371bde8ca1 0x7f371be4d718 0x7f371be4d833 0x7f371beedb2e 0x7f371beee394 0x7f371beee4e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f371de1cb97 0x5afa0a\n",
            "tcmalloc: large alloc 2734391296 bytes == 0x7f345276a000 @  0x7f371e21f1e7 0x7f371bde8ca1 0x7f371be4d718 0x7f371be4d833 0x7f371beedb2e 0x7f371beee394 0x7f371beee4e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f371de1cb97 0x5afa0a\n",
            "tcmalloc: large alloc 2661277696 bytes == 0x7f345276a000 @  0x7f371e21f1e7 0x7f371bde8ca1 0x7f371be4d718 0x7f371be4d833 0x7f371beedb2e 0x7f371beee394 0x7f371beee4e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f371de1cb97 0x5afa0a\n",
            "tcmalloc: large alloc 2243485696 bytes == 0x7f345276a000 @  0x7f371e21f1e7 0x7f371bde8ca1 0x7f371be4d718 0x7f371be4d833 0x7f371beedb2e 0x7f371beee394 0x7f371beee4e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f371de1cb97 0x5afa0a\n",
            "tcmalloc: large alloc 2017386496 bytes == 0x7f345276a000 @  0x7f371e21f1e7 0x7f371bde8ca1 0x7f371be4d718 0x7f371be4d833 0x7f371beedb2e 0x7f371beee394 0x7f371beee4e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f371de1cb97 0x5afa0a\n",
            "tcmalloc: large alloc 3270148096 bytes == 0x7f345276a000 @  0x7f371e21f1e7 0x7f371bde8ca1 0x7f371be4d718 0x7f371be4d833 0x7f371beedb2e 0x7f371beee394 0x7f371beee4e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f371de1cb97 0x5afa0a\n",
            "tcmalloc: large alloc 5098905600 bytes == 0x7f33228b8000 @  0x7f371e21f1e7 0x7f371bde8ca1 0x7f371be4d718 0x7f371be4d833 0x7f371beedb2e 0x7f371beee394 0x7f371beee4e2 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f371de1cb97 0x5afa0a\n",
            "2019-05-12 22:20:53.824639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-12 22:20:53.824703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-12 22:20:53.824716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-12 22:20:53.824732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-12 22:20:53.824887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Test loss: 1.182173\n",
            "Test accuracy: 0.81707317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_5WgIRs6YwZ",
        "colab_type": "code",
        "outputId": "57be4a4b-b0fd-476c-e84a-4f7d687d47d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1351
        }
      },
      "source": [
        "# Run sample\n",
        "!python3 src/main.py -r -c config.yaml -m src/test.mid"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "-------------------------------------------------------------------------------------\n",
            "Configuration: \n",
            "\tmodel:\n",
            "\t\ttype:\n",
            "\t\t\tNone\n",
            "\t\tsave_path:\n",
            "\t\t\tout/model\n",
            "\t\tparameters:\n",
            "\t\t\tbatch_size:\n",
            "\t\t\t\t128\n",
            "\t\t\tepochs:\n",
            "\t\t\t\t100\n",
            "\t\t\tlearning_rate:\n",
            "\t\t\t\t0.001\n",
            "\t\t\tbeta_1:\n",
            "\t\t\t\t0.9\n",
            "\t\t\tbeta_2:\n",
            "\t\t\t\t0.999\n",
            "\tdataset:\n",
            "\t\traw_path:\n",
            "\t\t\tdataset\n",
            "\t\tnum_class:\n",
            "\t\t\t10\n",
            "\tfeature_extraction:\n",
            "\t\tmin_note:\n",
            "\t\t\t0\n",
            "\t\tmax_note:\n",
            "\t\t\t127\n",
            "\t\tfeatures:\n",
            "\t\t\tNone\n",
            "\tdata_augmentation:\n",
            "\t\tsample_size:\n",
            "\t\t\t300\n",
            "\t\tnum_samples_per_track:\n",
            "\t\t\t60\n",
            "-------------------------------------------------------------------------------------\n",
            "pianorolls feature has been constructed\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2019-04-19 15:27:14.437044: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-04-19 15:27:14.437303: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2563340 executing computations on platform Host. Devices:\n",
            "2019-04-19 15:27:14.437340: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-04-19 15:27:14.643871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-04-19 15:27:14.644418: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2562f20 executing computations on platform CUDA. Devices:\n",
            "2019-04-19 15:27:14.644453: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-04-19 15:27:14.644869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-04-19 15:27:14.644902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-04-19 15:27:15.187032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-04-19 15:27:15.187114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-04-19 15:27:15.187127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-04-19 15:27:15.187389: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-04-19 15:27:15.187440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "None  model has been built\n",
            "Loaded model from file.\n",
            "tcmalloc: large alloc 1496064000 bytes == 0x259be000 @  0x7f99400551e7 0x7f993dc1fe51 0x7f993dc848d8 0x7f993dc849f3 0x7f993dd2484e 0x7f993dd250b4 0x7f993dd25202 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f993fc52b97 0x5afa0a\n",
            "2019-04-19 15:27:17.823190: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "Votes for artisits : \n",
            "\tchopin :  3505\n",
            "\tbartok :  19\n",
            "\thummel :  0\n",
            "\tother :  718\n",
            "\tmendelssohn :  0\n",
            "\tbach :  0\n",
            "\tbyrd :  0\n",
            "\thandel :  0\n",
            "\tschumann :  628\n",
            "\tmozart :  0\n",
            "Prediction is :  chopin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVeQOjgy6biR",
        "colab_type": "code",
        "outputId": "adf84903-e682-4e6d-e2ba-c3a24cefd326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2143
        }
      },
      "source": [
        "# Visualize\n",
        "!python3 src/main.py -v -c config.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "-------------------------------------------------------------------------------------\n",
            "Configuration: \n",
            "\tmodel:\n",
            "\t\ttype:\n",
            "\t\t\tNone\n",
            "\t\tsave_path:\n",
            "\t\t\tout/model\n",
            "\t\tparameters:\n",
            "\t\t\tbatch_size:\n",
            "\t\t\t\t128\n",
            "\t\t\tepochs:\n",
            "\t\t\t\t100\n",
            "\t\t\tlearning_rate:\n",
            "\t\t\t\t0.001\n",
            "\t\t\tbeta_1:\n",
            "\t\t\t\t0.9\n",
            "\t\t\tbeta_2:\n",
            "\t\t\t\t0.999\n",
            "\tdataset:\n",
            "\t\traw_path:\n",
            "\t\t\tdataset\n",
            "\t\tnum_class:\n",
            "\t\t\t10\n",
            "\tfeature_extraction:\n",
            "\t\tmin_note:\n",
            "\t\t\t0\n",
            "\t\tmax_note:\n",
            "\t\t\t127\n",
            "\t\tfeatures:\n",
            "\t\t\tNone\n",
            "\tdata_augmentation:\n",
            "\t\tsample_size:\n",
            "\t\t\t300\n",
            "\t\tnum_samples_per_track:\n",
            "\t\t\t60\n",
            "-------------------------------------------------------------------------------------\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "Midi files loaded : 506\n",
            "Different classes :  ['chopin', 'bartok', 'hummel', 'other', 'mendelssohn', 'bach', 'byrd', 'handel', 'schumann', 'mozart']\n",
            "Reading raw data...\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  RuntimeWarning)\n",
            "Midi files loaded : 506\n",
            "Different classes :  ['chopin', 'bartok', 'hummel', 'other', 'mendelssohn', 'bach', 'byrd', 'handel', 'schumann', 'mozart']\n",
            "Filter raw data...\n",
            "Preprocessing composer  chopin  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  bartok  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  hummel  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  other  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  mendelssohn  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  bach  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  byrd  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  handel  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  schumann  data\n",
            "pianorolls feature has been constructed\n",
            "Preprocessing composer  mozart  data\n",
            "pianorolls feature has been constructed\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n",
            "<Figure size 640x480 with 1 Axes>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7LQfNse7ypC",
        "colab_type": "code",
        "outputId": "2d0c0fc3-7e68-4066-ffef-77e3a09e9af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "!pip install pydrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "fid = '18zhap_EW80XFR81jg2O-AVGoIeCJW9gN'\n",
        "f = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": fid}]})\n",
        "f.SetContentFile( 'out/modelcnn.h5' )\n",
        "f.Upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.12.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzQrhyOTO7Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv dataset/chopin/chopin049.mid ./\n",
        "!mv dataset/chopin/chopin050.mid ./\n",
        "!mv dataset/chopin/chopin051.mid ./\n",
        "!mv dataset/chopin/chopin052.mid ./\n",
        "!rm -r dataset/chopin/*\n",
        "!mv chopin049.mid dataset/chopin\n",
        "!mv chopin050.mid dataset/chopin\n",
        "!mv chopin051.mid dataset/chopin\n",
        "!mv chopin052.mid dataset/chopin\n",
        "\n",
        "!mv dataset/bach/bach342.mid ./\n",
        "!mv dataset/bach/bach343.mid ./\n",
        "!mv dataset/bach/bach344.mid ./\n",
        "!mv dataset/bach/bach345.mid ./\n",
        "!rm -r dataset/bach/*\n",
        "!mv bach342.mid dataset/bach\n",
        "!mv bach343.mid dataset/bach\n",
        "!mv bach344.mid dataset/bach\n",
        "!mv bach345.mid dataset/bach\n",
        "\n",
        "!mv dataset/bartok/bartok392.mid ./\n",
        "!mv dataset/bartok/bartok393.mid ./\n",
        "!mv dataset/bartok/bartok394.mid ./\n",
        "!mv dataset/bartok/bartok395.mid ./\n",
        "!rm -r dataset/bartok/*\n",
        "!mv bartok392.mid dataset/bartok\n",
        "!mv bartok393.mid dataset/bartok\n",
        "!mv bartok394.mid dataset/bartok\n",
        "!mv bartok395.mid dataset/bartok\n",
        "\n",
        "!mv dataset/handel/handel099.mid ./\n",
        "!mv dataset/handel/handel100.mid ./\n",
        "!mv dataset/handel/handel101.mid ./\n",
        "!mv dataset/handel/handel102.mid ./\n",
        "!rm -r dataset/handel/*\n",
        "!mv handel099.mid dataset/handel\n",
        "!mv handel100.mid dataset/handel\n",
        "!mv handel101.mid dataset/handel\n",
        "!mv handel102.mid dataset/handel\n",
        "\n",
        "!mv dataset/byrd/byrd149.mid ./\n",
        "!mv dataset/byrd/byrd150.mid ./\n",
        "!mv dataset/byrd/byrd151.mid ./\n",
        "!mv dataset/byrd/byrd152.mid ./\n",
        "!rm -r dataset/byrd/*\n",
        "!mv byrd149.mid dataset/byrd\n",
        "!mv byrd150.mid dataset/byrd\n",
        "!mv byrd151.mid dataset/byrd\n",
        "!mv byrd152.mid dataset/byrd\n",
        "\n",
        "!mv dataset/hummel/hummel292.mid ./\n",
        "!mv dataset/hummel/hummel293.mid ./\n",
        "!mv dataset/hummel/hummel294.mid ./\n",
        "!mv dataset/hummel/hummel295.mid ./\n",
        "!rm -r dataset/hummel/*\n",
        "!mv hummel292.mid dataset/hummel\n",
        "!mv hummel293.mid dataset/hummel\n",
        "!mv hummel294.mid dataset/hummel\n",
        "!mv hummel295.mid dataset/hummel\n",
        "\n",
        "!mv dataset/mendelssohn/mendelssohn243.mid ./\n",
        "!mv dataset/mendelssohn/mendelssohn244.mid ./\n",
        "!mv dataset/mendelssohn/mendelssohn245.mid ./\n",
        "!mv dataset/mendelssohn/mendelssohn246.mid ./\n",
        "!rm -r dataset/mendelssohn/*\n",
        "!mv mendelssohn243.mid dataset/mendelssohn\n",
        "!mv mendelssohn244.mid dataset/mendelssohn\n",
        "!mv mendelssohn245.mid dataset/mendelssohn\n",
        "!mv mendelssohn246.mid dataset/mendelssohn\n",
        "\n",
        "!mv dataset/mozart/mozart000.mid ./\n",
        "!mv dataset/mozart/mozart001.mid ./\n",
        "!mv dataset/mozart/mozart002.mid ./\n",
        "!mv dataset/mozart/mozart003.mid ./\n",
        "!rm -r dataset/mozart/*\n",
        "!mv mozart000.mid dataset/mozart\n",
        "!mv mozart001.mid dataset/mozart\n",
        "!mv mozart002.mid dataset/mozart\n",
        "!mv mozart003.mid dataset/mozart\n",
        "\n",
        "!mv dataset/other/appass_1.mid ./\n",
        "!mv dataset/other/appass_2.mid ./\n",
        "!mv dataset/other/appass_3.mid ./\n",
        "!mv dataset/other/elise.mid ./\n",
        "!rm -r dataset/other/*\n",
        "!mv appass_1.mid dataset/other\n",
        "!mv appass_2.mid dataset/other\n",
        "!mv appass_3.mid dataset/other\n",
        "!mv elise.mid dataset/other\n",
        "\n",
        "!mv dataset/schumann/schumann199.mid ./\n",
        "!mv dataset/schumann/schumann200.mid ./\n",
        "!mv dataset/schumann/schumann201.mid ./\n",
        "!mv dataset/schumann/schumann202.mid ./\n",
        "!rm -r dataset/schumann/*\n",
        "!mv schumann199.mid dataset/schumann\n",
        "!mv schumann200.mid dataset/schumann\n",
        "!mv schumann201.mid dataset/schumann\n",
        "!mv schumann202.mid dataset/schumann"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x85utFhbWLsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
